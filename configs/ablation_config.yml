data:
  dataset: "CIFAR10"  # Dataset to use (CIFAR10, LSUN, CelebA, etc.)
  image_size: 32  # Image size
  channels: 3  # Number of channels
  num_workers: 4  # Number of workers for data loading
  random_flip: True  # Whether to randomly flip the images

model:
  type: "simple"  # Model type (simple or ddpm)
  in_channels: 3  # Input channels
  out_channels: 3  # Output channels
  ch: 128  # Base channel count
  ch_mult: [1, 2, 2, 2]  # Channel multiplier (determines depth)
  num_res_blocks: 2  # Number of residual blocks per resolution
  attn_resolutions: [16]  # Resolutions at which to apply attention
  dropout: 0.1  # Dropout rate
  var_type: "fixedlarge"  # Variance type (fixedlarge or fixedsmall)
  ema_rate: 0.9999  # EMA rate for weight averaging
  ema: True  # Whether to use EMA
  time_embed_dim: 128  # Time embedding dimension

diffusion:
  beta_schedule: "linear"  # Beta schedule type
  beta_start: 0.0001  # Beta start value
  beta_end: 0.02  # Beta end value
  num_diffusion_timesteps: 1000  # Number of diffusion timesteps

training:
  batch_size: 32  # Batch size
  n_epochs: 10000  # Number of epochs
  n_iters: 5000000  # Number of iterations
  snapshot_freq: 5000  # Snapshot frequency
  validation_freq: 2000  # Validation frequency
  
optim:
  weight_decay: 0.0  # Weight decay
  optimizer: "Adam"  # Optimizer to use
  lr: 0.0002  # Learning rate
  beta1: 0.9  # Beta1 for Adam
  eps: 1.0e-8  # Epsilon for Adam
  grad_clip: 1.0  # Gradient clipping value

sampling:
  batch_size: 16  # Batch size for sampling
  last_only: True  # Whether to only save the last sample during sampling
  clip_denoised: True  # Whether to clip denoised values
  sampling_timesteps: 1000  # Number of sampling timesteps 